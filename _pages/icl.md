---
permalink: /icl/
title: "ICL papers"
#excerpt: "Collection of ICL papers"
author_profile: false 
---

## Theory 

Von Oswald, Johannes, Eyvind Niklasson, Ettore Randazzo, Jo√£o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. "Transformers learn in-context by gradient descent." In International Conference on Machine Learning, pp. 35151-35174. PMLR, 2023. [[Paper]](https://proceedings.mlr.press/v202/von-oswald23a/von-oswald23a.pdf)

Ahn, Kwangjun, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. "Transformers learn to implement preconditioned gradient descent for in-context learning." Advances in Neural Information Processing Systems 36 (2024). [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/8ed3d610ea4b68e7afb30ea7d01422c6-Paper-Conference.pdf)

Fu, Deqing, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. "Transformers learn higher-order optimization methods for in-context learning: A study with linear models." arXiv preprint arXiv:2310.17086 (2023). [[Paper]](https://arxiv.org/pdf/2310.17086)

